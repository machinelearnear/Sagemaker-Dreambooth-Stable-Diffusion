{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f4d58b-ab6f-4713-b6fa-26dcd4b67e46",
   "metadata": {},
   "source": [
    "# Running Dreambooth Stable Diffusion on Sagemaker\n",
    "- https://github.com/XavierXiao/Dreambooth-Stable-Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e88a13-49ba-47e9-8fef-c5b7de8df155",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04d6bf-8eae-4b59-9ae0-c3b2899a4502",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523536a-fadc-46c8-9527-c3a1a5df42cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists as path_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf4977-9977-4da6-8a8b-6c8a7daa0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b008a-0ec7-4668-af50-5f74ec444b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA available.')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('CUDA not available. Please connect to a GPU instance if possible.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ba143-eb82-4451-89c7-b1f716342f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6b287-b70f-4943-97c0-255cad192ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'Dreambooth-Stable-Diffusion' in os.getcwd():\n",
    "    if not path_exists('Dreambooth-Stable-Diffusion'):\n",
    "        os.system(\"git clone https://github.com/XavierXiao/Dreambooth-Stable-Diffusion\")\n",
    "    os.chdir(\"Dreambooth-Stable-Diffusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485283f-4944-4347-8fe7-757809561e4a",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12003ecd-af97-4239-a0db-423d1b7d15c5",
   "metadata": {},
   "source": [
    "[@XavierXiao](https://github.com/XavierXiao/Dreambooth-Stable-Diffusion) wrote:\n",
    "> To fine-tune a stable diffusion model, you need to obtain the pre-trained stable diffusion models following their instructions. Weights can be downloaded on HuggingFace. You can decide which version of checkpoint to use, but I use `sd-v1-4-full-ema.ckpt`.\n",
    "\n",
    "> We also need to create a set of images for regularization, as the fine-tuning algorithm of Dreambooth requires that. Details of the algorithm can be found in the paper. Note that in the original paper, the regularization images seem to be generated on-the-fly. However, here I generated a set of regularization images before the training. The text prompt for generating regularization images can be photo of a `<class>`, where `<class>` is a word that describes the class of your object, such as dog. The command is:\n",
    "\n",
    "```bash\n",
    "python scripts/stable_txt2img.py --ddim_eta 0.0 --n_samples 8 --n_iter 1 --scale 10.0 --ddim_steps 50 --ckpt /path/to/original/stable-diffusion/sd-v1-4-full-ema.ckpt --prompt \"a photo of a <class>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa687a3-6de4-4aa7-bed4-93b851d1545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_word = \"yerba mate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628800c-cace-4b86-b036-901ccd27b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim_eta = 0.0\n",
    "n_samples = 8\n",
    "n_iter = 1\n",
    "scale = 10.0\n",
    "ddim_steps = 50\n",
    "ckpt = '/path/to/original/stable-diffusion/sd-v1-4-full-ema.ckpt'\n",
    "prompt = f\"a photo of a {class_word}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40c52b-db94-41ab-911f-76e22d2973ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"python scripts/stable_txt2img.py --ddim_eta {ddim_eta} \" + \n",
    "          f\"--n_samples {n_samples} --n_iter {n_iter} --scale {scale} \" + \n",
    "          f\"--ddim_steps {ddim_steps} --ckpt {ckpt} --prompt {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f004b68-4680-464b-99fe-151fbad67d70",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8964bbba-a8ab-4727-9df0-5faa8563910a",
   "metadata": {},
   "source": [
    "[@XavierXiao](https://github.com/XavierXiao/Dreambooth-Stable-Diffusion) wrote:\n",
    "> Training can be done by running the following command\n",
    "\n",
    "```bash\n",
    "python main.py --base configs/stable-diffusion/v1-finetune_unfrozen.yaml \n",
    "                -t \n",
    "                --actual_resume /path/to/original/stable-diffusion/sd-v1-4-full-ema.ckpt  \n",
    "                -n <job name> \n",
    "                --gpus 0, \n",
    "                --data_root /root/to/training/images \n",
    "                --reg_data_root /root/to/regularization/images \n",
    "                --class_word <xxx>\n",
    "```\n",
    "\n",
    "> Detailed configuration can be found in `configs/stable-diffusion/v1-finetune_unfrozen.yaml`. In particular, the default learning rate is `1.0e-6` as I found the `1.0e-5` in the Dreambooth paper leads to poor editability. The parameter `reg_weight` corresponds to the weight of regularization in the Dreambooth paper, and the default is set to `1.0`.\n",
    "\n",
    "> Dreambooth requires a placeholder word [V], called identifier, as in the paper. This identifier needs to be a relatively rare tokens in the vocabulary. The original paper approaches this by using a rare word in `T5-XXL` tokenizer. For simplicity, here I just use a random word sks and hard coded it.. If you want to change that, simply make a change in this file.\n",
    "\n",
    "> Training will be run for 800 steps, and two checkpoints will be saved at `./logs/<job_name>/checkpoints`, one at 500 steps and one at final step. Typically the one at 500 steps works well enough. I train the model use two A6000 GPUs and it takes ~15 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1e6ee-2d9b-49af-bdd8-84240f95058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"configs/stable-diffusion/v1-finetune_unfrozen.yaml\"\n",
    "n = 'test'\n",
    "gpus = 0,\n",
    "data_root = \"/root/to/training/images\"\n",
    "reg_data_root = '/path/to/regularization/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6007b3-f923-49ae-9d61-23faf18b8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"python main.py --base {base} -t --actual_resume {ckpt} -n {n} \" + \n",
    "          f\"--gpus {gpus} --data_root {data_root} --reg_data_root {reg_data_root} --class_word {class_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ffd87-d0c9-459d-8bb6-66a5a8978943",
   "metadata": {},
   "source": [
    "## Image generation from saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408c5fa-5758-408a-b960-7ae47c8819ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_ckpt = \"/path/to/saved/checkpoint/from/training\"\n",
    "new_prompt = f\"photo of a wooden {class_word}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d8759-3c88-4d71-8cd2-4de18b9a345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"python scripts/stable_txt2img.py --ddim_eta {ddim_eta} \" + \n",
    "          f\"--n_samples {n_samples} --n_iter {n_iter} --scale {scale} \" + \n",
    "          f\"--ddim_steps {ddim_steps} --ckpt {saved_ckpt} --prompt {new_prompt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearnear-default:Python",
   "language": "python",
   "name": "conda-env-machinelearnear-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
